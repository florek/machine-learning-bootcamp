# Gradient Descent â€“ prosta regresja liniowa (krowie na rowie)

Ten plik tÅ‚umaczy **linijka po linijce**, co dokÅ‚adnie dzieje siÄ™ w poniÅ¼szym kodzie. To jest **rÄ™czna implementacja regresji liniowej** uczonej metodÄ… **gradient descent**.

---

## 1. Import bibliotek

```python
import numpy as np
import pandas as pd
import plotly.express as px
```

* **NumPy** â€“ liczenie wektorÃ³w, macierzy, pochodnych
* **Pandas** â€“ trzymanie historii uczenia w tabeli
* **Plotly** â€“ rysowanie wykresÃ³w (jak parametry siÄ™ uczÄ…)

---

## 2. Dane wejÅ›ciowe

```python
X1 = np.array([1, 2, 3, 4, 5, 6])
Y = np.array([3000, 3250, 3500, 3750, 4000, 4250])
```

Interpretacja:

* **X1** â†’ lata pracy
* **Y** â†’ wynagrodzenie

ZakÅ‚adamy model:

> im wiÄ™cej lat pracy, tym wiÄ™ksza pensja

---

## 3. Liczba prÃ³bek

```python
m = len(X1)
```

* `m = 6`
* tyle mamy obserwacji (punktÃ³w danych)

---

## 4. Zmiana ksztaÅ‚tu danych (reshape)

```python
X1 = X1.reshape(m, 1)
Y = Y.reshape(-1, 1)
```

Dlaczego?

* gradient descent **operuje na macierzach**
* chcemy mieÄ‡ kolumny, a nie listy

Efekt:

* `X1.shape == (6, 1)`
* `Y.shape == (6, 1)`

---

## 5. Dodanie biasu (wyraz wolny)

```python
bias = np.ones((m, 1))
X = np.append(bias, X1, axis=1)
```

Bias = kolumna jedynek:

```
[1, 1]
[1, 2]
[1, 3]
[1, 4]
[1, 5]
[1, 6]
```

Dlaczego?

Model matematyczny:

```
Y = w0 * 1 + w1 * X
```

* `w0` â†’ intercept (punkt startowy)
* `w1` â†’ wspÃ³Å‚czynnik (nachylenie prostej)

---

## 6. Parametry uczenia

```python
eta = 0.01
weights = np.random.randn(2, 1)
```

* `eta` â†’ learning rate (jak duÅ¼y krok robimy)
* `weights` â†’ losowy start:

  * `weights[0]` â†’ intercept
  * `weights[1]` â†’ wspÃ³Å‚czynnik przy X

---

## 7. Gradient Descent â€“ serce algorytmu

```python
for i in range(3000):
    gradient = (2 / m) * X.T.dot(X.dot(weights) - Y)
    weights = weights - eta * gradient
```

Co tu siÄ™ dzieje:

### a) Predykcja

```
X.dot(weights)
```

â†’ aktualne przewidywane pensje

### b) BÅ‚Ä…d

```
X.dot(weights) - Y
```

â†’ o ile siÄ™ mylimy dla kaÅ¼dego punktu

### c) Gradient

```
(2 / m) * X.T.dot(bÅ‚Ä…d)
```

â†’ kierunek, w ktÃ³rym trzeba **zmniejszyÄ‡ bÅ‚Ä…d MSE**

### d) Aktualizacja wag

```
weights = weights - eta * gradient
```

â†’ robimy maÅ‚y krok w dÃ³Å‚ zbocza bÅ‚Ä™du

---

## 8. Zapisywanie historii uczenia

```python
intercept.append(weights[0][0])
coef.append(weights[1][0])
```

Po co?

* Å¼eby **zobaczyÄ‡, jak model siÄ™ uczy**
* jak stabilizujÄ… siÄ™ parametry

---

## 9. Wynik koÅ„cowy

```python
print(weights)
```

To jest gotowy model:

```
Y = intercept + coef * X
```

---

## 10. DataFrame z historiÄ…

```python
df = pd.DataFrame({
    'intercept': intercept,
    'coef': coef
})
```

KaÅ¼dy wiersz = jeden krok gradient descent

---

## 11. Wizualizacja uczenia

### Intercept

```python
px.line(df, y='intercept')
```

â†’ pokazuje, jak stabilizuje siÄ™ punkt przeciÄ™cia z osiÄ… Y

### WspÃ³Å‚czynnik

```python
px.line(df, y='coef')
```

â†’ pokazuje, jak zmienia siÄ™ nachylenie prostej

---

## 12. TL;DR (mega skrÃ³t)

* masz dane: lata pracy â†’ pensja
* zgadujesz losowÄ… prostÄ…
* liczysz, jak bardzo siÄ™ myli
* poprawiasz prostÄ… **3000 razy**
* na koÅ„cu dostajesz sensowny model

To jest **dokÅ‚adnie to**, co robi sklearn â€“ tylko bez magii ğŸ¯
