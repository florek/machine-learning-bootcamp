# Klucz odpowiedzi – 12.02.2026

| Pytanie | Poprawna | Krótkie uzasadnienie |
|---------|----------|----------------------|
| 1 | A | W gradient descent wagi aktualizuje się odejmując krok (eta * gradient), aby minimalizować błąd. |
| 2 | B | Znacznie wyższy score na train niż na test oznacza overfitting (przeuczenie). |
| 3 | C | drop_first=True unika pułapki zmiennych fikcyjnych i redukuje kolinearność. |
| 4 | D | test_size=0.25 oznacza 25% próbek w zbiorze testowym. |
| 5 | A | MAE jest w jednostkach zmiennej docelowej i jest czytelna biznesowo. |
| 6 | B | W OLS zmienna istotna statystycznie gdy p-value < poziomu istotności (np. 0.05). |
| 7 | C | Kolumna jedynek to wyraz wolny (intercept) w równaniu Y = w0·1 + w1·X. |
| 8 | D | Parametr noise w make_regression określa poziom szumu dodawanego do targetu. |
| 9 | A | Gradient wskazuje kierunek i wielkość kroku, by zmniejszyć błąd (MSE). |
| 10 | B | W backward elimination usuwa się zmienną z najwyższym p-value > poziomu istotności. |
| 11 | C | np.argmax zwraca indeks elementu maksymalnego (indeks kolumny do usunięcia). |
| 12 | D | Regresja wielomianowa = PolynomialFeatures (1, X, X², …) + LinearRegression. |
| 13 | A | W statsmodels kolumnę stałej dodaje się przez sm.add_constant(). |
| 14 | B | W równaniu Y = w0·1 + w1·X parametr w0 to wyraz wolny (intercept). |
| 15 | C | Dobry model ma błędy w przybliżeniu symetryczne wokół zera (rozkład normalny). |
| 16 | D | Forward Selection startuje od pustego modelu i dodaje zmienne po kolei. |
| 17 | A | Wysoki stopień przy małej liczbie danych grozi przeuczeniem; zaleca się regularyzację lub niższy stopień. |
| 18 | B | pop() usuwa kolumnę z DataFrame i zwraca ją jako Series. |
| 19 | C | stratify=y zachowuje proporcje klas w zbiorach train i test. |
| 20 | D | R² = 0 oznacza, że model nie wyjaśnia wariancji lepiej niż średnia. |
