# Klucz odpowiedzi – 13.02.2026

| Pytanie | Poprawna | Krótkie uzasadnienie |
|---------|----------|----------------------|
| 1 | C | Przy outlierach średnia zniekształca imputację; dokumentacja zaleca median. |
| 2 | A | W gradient descent wagi aktualizuje się: weights = weights - eta * gradient. |
| 3 | C | Pętla kończy się, gdy wszystkie zmienne mają p-value ≤ sl (np. 0.05). |
| 4 | B | random_state zapewnia powtarzalność podziału (te same train/test przy każdym uruchomieniu). |
| 5 | C | statsmodels wymaga numpy float; boolean z get_dummies() konwertuje się na 1.0/0.0. |
| 6 | A | Równanie: y = intercept_ + coef_[0]*x1 + coef_[1]*x2 + … |
| 7 | D | Stepwise to kombinacja forward i backward – można dodawać i usuwać zmienne. |
| 8 | B | corr() zwraca macierz współczynników korelacji Pearsona między kolumnami. |
| 9 | C | Duża różnica score train vs test to overfitting; rozważ prostszy model lub regularyzację. |
| 10 | C | pop() usuwa kolumnę z DataFrame i zwraca ją; cechy i target trzymane osobno. |
| 11 | D | W θ = (XᵀX)⁻¹Xᵀy theta[0] to intercept, theta[1] to współczynnik dla pierwszej cechy. |
| 12 | B | drop_first=True unika dummy variable trap i redukuje kolinearność. |
| 13 | B | MAE jest w tych samych jednostkach co zmienna docelowa. |
| 14 | A | scikit-learn oczekuje macierzy 2D (próbki × cechy); reshape(n, 1) daje jedną kolumnę. |
| 15 | C | Regularyzacja (Ridge/Lasso) lub niższy stopień zmniejsza ryzyko przeuczenia. |
| 16 | D | Gradient MSE dla regresji liniowej: (2/m) * X.T.dot(X.dot(weights) - Y). |
| 17 | C | W OLS zmienna nieistotna statystycznie gdy p-value ≥ 0.05. |
| 18 | A | test_size=0.2 oznacza 20% danych do zbioru testowego, 80% do treningu. |
| 19 | B | ols.save('model.pickle') zapisuje dopasowany model OLS; potem pickle.load(). |
| 20 | B | Dobry model ma błędy w przybliżeniu symetryczne wokół zera (rozkład normalny). |
