# Klucz odpowiedzi – 11.02.2026

| Pytanie | Poprawna | Krótkie uzasadnienie |
|---------|----------|----------------------|
| 1 | A | W gradient descent wagi aktualizuje się odejmując krok (eta * gradient), aby minimalizować błąd. |
| 2 | B | Znacznie wyższy R² na train niż na test oznacza przeuczenie (overfitting). |
| 3 | C | drop_first=True usuwa jedną kategorię, co unika dummy variable trap i redukuje kolinearność. |
| 4 | D | test_size=0.25 oznacza 25% próbek przeznaczonych na zbiór testowy. |
| 5 | C | MAE jest w tych samych jednostkach co target (np. dolary), w odróżnieniu od R² czy MSE. |
| 6 | C | W OLS zmienna jest istotna statystycznie, gdy p-value < przyjętego poziomu (np. 0.05). |
| 7 | B | Kolumna jedynek to wyraz wolny (intercept) w równaniu Y = w0·1 + w1·X. |
| 8 | A | Parametr noise w make_regression określa poziom szumu dodawanego do zmiennej docelowej. |
| 9 | A | Gradient wskazuje kierunek wzrostu błędu; odejmując go od wag, zmniejszamy błąd. |
| 10 | D | W backward elimination usuwa się zmienną z najwyższym p-value powyżej poziomu istotności. |
| 11 | B | np.argmax zwraca indeks elementu maksymalnego, czyli indeks kolumny do usunięcia. |
| 12 | B | Regresja wielomianowa = PolynomialFeatures (1, X, X², …) + zwykła LinearRegression. |
| 13 | C | W statsmodels kolumnę stałej dodaje się przez sm.add_constant(). |
| 14 | A | W równaniu Y = w0·1 + w1·X parametr w0 to intercept (punkt przecięcia z osią Y). |
| 15 | D | Dobry model ma błędy zbliżone do rozkładu normalnego wokół zera. |
| 16 | D | Forward Selection startuje od pustego modelu i dodaje zmienne (odwrotnie niż backward). |
| 17 | A | Wysoki stopień przy małej liczbie danych grozi przeuczeniem; zaleca się regularyzację lub niższy stopień. |
| 18 | D | pop() usuwa kolumnę z DataFrame i zwraca ją jako Series. |
| 19 | C | stratify=y zachowuje proporcje klas w podzbiorach train i test. |
| 20 | B | R² = 0 oznacza, że model nie wyjaśnia wariancji lepiej niż średnia. |
