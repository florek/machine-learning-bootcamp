# Quiz ML Bootcamp – 11.02.2026

---

## Pytanie 1 [łatwe]

W metodzie gradient descent wagi są aktualizowane w każdej iteracji. Który wzór poprawnie opisuje ten krok?

A) `weights = weights - eta * gradient`  
B) `weights = weights + eta * gradient`  
C) `weights = eta * gradient`  
D) `weights = weights / (eta * gradient)`

---

## Pytanie 2 [średnie]

W regresji liniowej na danych insurance po podziale train/test porównujesz `regressor.score(X_train, y_train)` i `regressor.score(X_test, y_test)`. Co oznacza sytuacja, gdy score na zbiorze treningowym jest znacznie wyższy niż na testowym?

A) Model jest niedoucowany (underfitting).  
B) Model jest przeuczony (overfitting).  
C) Podział danych był nieprawidłowy.  
D) Metryka R² nie nadaje się do oceny.

---

## Pytanie 3 [średnie]

Dlaczego w one-hot encoding stosuje się `drop_first=True` w `pd.get_dummies()`?

A) Aby usunąć pierwszą kolumnę alfabetycznie.  
B) Aby przyspieszyć trenowanie modelu.  
C) Aby uniknąć pułapki zmiennych fikcyjnych i redukować kolinearność.  
D) Aby zmniejszyć liczbę wierszy w zbiorze danych.

---

## Pytanie 4 [łatwe]

W wywołaniu `train_test_split(X, y, test_size=0.25)` parametr `test_size=0.25` oznacza:

A) 25% danych trafia do zbioru treningowego.  
B) Liczbę 25 próbek w zbiorze testowym.  
C) 25% danych trafia do zbioru walidacyjnego.  
D) 25% danych trafia do zbioru testowego.

---

## Pytanie 5 [łatwe]

Która metryka jest wyrażona w tych samych jednostkach co zmienna docelowa (np. dolary) i jest szczególnie czytelna w kontekście biznesowym?

A) Mean Squared Error (MSE).  
B) Współczynnik determinacji R².  
C) Mean Absolute Error (MAE).  
D) Wartość p-value.

---

## Pytanie 6 [średnie]

W modelu OLS (statsmodels) zmienna uznawana jest za istotną statystycznie, gdy:

A) p-value > 0.05.  
B) p-value ≥ 0.05.  
C) p-value < 0.05.  
D) p-value = 0.

---

## Pytanie 7 [średnie]

W ręcznej implementacji gradient descent do macierzy cech dodawana jest kolumna jedynek. Jaki jest jej cel?

A) Zwiększa liczbę iteracji potrzebnych do zbieżności.  
B) Reprezentuje wyraz wolny (intercept) w równaniu Y = w0·1 + w1·X.  
C) Zastępuje pierwszą cechę.  
D) Służy do normalizacji cech.

---

## Pytanie 8 [trudne]

W `make_regression(n_samples=100, n_features=1, n_informative=1, noise=10, random_state=42)` parametr `noise=10` określa:

A) Poziom szumu dodawanego do zmiennej docelowej (rozrzut wokół prawdziwej zależności).  
B) Liczbę cech szumowych.  
C) Liczbę obserwacji z brakującymi wartościami.  
D) Ziarno generatora liczb losowych.

---

## Pytanie 9 [łatwe]

W gradient descent w każdej iteracji obliczany jest gradient. Do czego służy ten gradient?

A) Określa kierunek i wielkość kroku, w którym należy zaktualizować wagi, aby zmniejszyć błąd.  
B) Określa końcową wartość R².  
C) Służy do podziału danych na train i test.  
D) Zastępuje learning rate (eta).

---

## Pytanie 10 [trudne]

W automatycznej backward elimination (pętla z OLS) w każdej iteracji usuwa się zmienną, która:

A) Ma najniższy współczynnik regresji.  
B) Ma najniższe p-value.  
C) Ma najwyższą korelację z targetem.  
D) Ma najwyższe p-value spośród zmiennych nieistotnych (p > poziom istotności).

---

## Pytanie 11 [średnie]

Funkcja `np.argmax(ols.pvalues.astype('float'))` w kontekście backward elimination zwraca:

A) Maksymalną wartość p-value.  
B) Indeks zmiennej (kolumny) o najwyższym p-value.  
C) Liczbę zmiennych istotnych statystycznie.  
D) Listę nazw zmiennych do usunięcia.

---

## Pytanie 12 [średnie]

Regresja wielomianowa w scikit-learn polega na:

A) Użyciu osobnego algorytmu „PolynomialRegression”.  
B) Rozszerzeniu cech (np. 1, X, X², X³) przez PolynomialFeatures i dopasowaniu zwykłej regresji liniowej do tych cech.  
C) Zastąpieniu LinearRegression modelem nieliniowym.  
D) Tylko na zwiększeniu stopnia bez zmiany liczby cech.

---

## Pytanie 13 [łatwe]

Przed dopasowaniem modelu OLS w statsmodels do macierzy cech dodaje się kolumnę stałej. Jaką funkcję się do tego stosuje?

A) `np.ones()`.  
B) `pd.get_dummies()`.  
C) `sm.add_constant()`.  
D) `np.append()`.

---

## Pytanie 14 [trudne]

W równaniu modelu Y = w0·1 + w1·X (gradient descent) parametr w0 oznacza:

A) Wyraz wolny (intercept), czyli punkt przecięcia z osią Y.  
B) Współczynnik kierunkowy (nachylenie prostej).  
C) Learning rate.  
D) Liczbę iteracji.

---

## Pytanie 15 [średnie]

Histogram błędów predykcji (y_test - y_pred) dobrze dopasowanego modelu regresji powinien:

A) Być płaski.  
B) Mieć jeden ostry pik poza zerem.  
C) Być silnie asymetryczny.  
D) Być w przybliżeniu symetryczny wokół zera, o kształcie zbliżonym do rozkładu normalnego.

---

## Pytanie 16 [trudne]

W metodzie Forward Selection (selekcja zmiennych) w odróżnieniu od Backward Elimination:

A) Startuje się z pełnym modelem i usuwa zmienne.  
B) Używa się tylko p-value dla intercept.  
C) Nie stosuje się poziomu istotności.  
D) Startuje się od pustego modelu i dodaje zmienne po kolei.

---

## Pytanie 17 [trudne]

Przy regresji wielomianowej z wysokim stopniem wielomianu i stosunkowo małą liczbą obserwacji dokumentacja zaleca:

A) Rozważyć regularyzację (np. Ridge, Lasso) lub obniżenie stopnia wielomianu ze względu na ryzyko przeuczenia.  
B) Zawsze używać degree=2.  
C) Zwiększyć liczbę cech wejściowych.  
D) Pominąć train/test split.

---

## Pytanie 18 [średnie]

Wyrażenie `target = data.pop('charges')` w kontekście przygotowania danych do modelu:

A) Dodaje kolumnę 'charges' do DataFrame.  
B) Zwraca pierwszą wartość kolumny 'charges'.  
C) Tworzy kopię kolumny bez usuwania z data.  
D) Usuwa kolumnę 'charges' z DataFrame i zwraca ją jako Series (target); zapobiega użyciu targetu jako cechy.

---

## Pytanie 19 [łatwe]

Parametr `stratify=y` w `train_test_split()` służy do:

A) Zwiększenia rozmiaru zbioru testowego.  
B) Ustawienia ziarna generatora.  
C) Zachowania proporcji klas (np. smoker/nie) w zbiorze treningowym i testowym.  
D) Usunięcia duplikatów przed podziałem.

---

## Pytanie 20 [średnie]

Współczynnik determinacji R² = 0 dla modelu regresji oznacza, że:

A) Model idealnie dopasowuje dane.  
B) Model nie wyjaśnia wariancji lepiej niż prosta średnia.  
C) Wszystkie współczynniki regresji są równe zero.  
D) Błąd predykcji jest maksymalny.
