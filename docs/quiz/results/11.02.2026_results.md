# Wynik quizu — 11.02.2026

## Podsumowanie
- Wynik: 16/20
- Procent: 80%
- Braki: brak
- Najczęstsze kategorie błędów: średnie (3), trudne (1)
- Rekomendacje do powtórki:
  1. Interpretacja metryk i overfitting (różnice między score na train i test).
  2. Szczegóły implementacji selekcji zmiennych (backward/forward elimination, `np.argmax`, logika usuwania kolumn).
  3. Koncepcja regresji wielomianowej i kiedy stosować rozszerzenie cech.

## Szczegóły błędów

### Pytanie 2 [średnie]
**Krótka treść:** Co oznacza, gdy `regressor.score(X_train, y_train)` jest znacznie wyższy niż `regressor.score(X_test, y_test)`?  
**Twoja odpowiedź:** A  
**Poprawna odpowiedź:** B  
**Komentarz edukacyjny:** Znacznie wyższy wynik na zbiorze treningowym niż na testowym oznacza przeuczenie (overfitting) – model zapamiętuje dane treningowe zamiast uczyć się ogólnych wzorców. Underfitting występuje, gdy oba wyniki są niskie, czyli model jest zbyt prosty. Kluczowe jest zawsze porównanie score na train i test, a nie patrzenie tylko na jeden z nich. To pytanie sprawdza praktyczną interpretację metryk w kontekście generalizacji.

### Pytanie 11 [średnie]
**Krótka treść:** Co zwraca `np.argmax(ols.pvalues.astype('float'))` w backward elimination?  
**Twoja odpowiedź:** A  
**Poprawna odpowiedź:** B  
**Komentarz edukacyjny:** `np.argmax` zwraca indeks elementu o największej wartości, a nie samą wartość. W pętli backward elimination ten indeks wskazuje kolumnę (zmienną), którą należy usunąć z macierzy cech. Mylenie „maksymalnej wartości” z „indeksem maksymalnej wartości” to typowy błąd – w kodzie zawsze zwracaj uwagę, czy funkcja daje wartość, czy pozycję.

### Pytanie 12 [średnie]
**Krótka treść:** Na czym polega regresja wielomianowa w scikit-learn?  
**Twoja odpowiedź:** C  
**Poprawna odpowiedź:** B  
**Komentarz edukacyjny:** W scikit-learn regresja wielomianowa to zwykła regresja liniowa na rozszerzonych cechach wygenerowanych przez `PolynomialFeatures` (1, X, X², X³, …). Nie ma osobnego algorytmu „PolynomialRegression”, a model nadal jest liniowy względem parametrów – nieliniowość wynika wyłącznie z transformacji cech. Pytanie celuje w zrozumienie architektury scikit-learn i unikanie mylenia typu modelu z transformacją danych.

### Pytanie 16 [trudne]
**Krótka treść:** Jak w praktyce działa Forward Selection w porównaniu z Backward Elimination?  
**Twoja odpowiedź:** A  
**Poprawna odpowiedź:** D  
**Komentarz edukacyjny:** Forward Selection startuje od pustego modelu i dodaje zmienne po jednej, wybierając w każdym kroku tę, która najbardziej poprawia model (np. istotność statystyczna, kryterium informacyjne). Backward Elimination robi odwrotnie – startuje z pełnego modelu i usuwa najmniej istotne zmienne. To pytanie sprawdza, czy rozróżniasz strategię „dodajemy od zera” od „usuwamy z pełnego modelu”, co jest ważne przy projektowaniu pipeline’u selekcji cech.

## Tabela porównawcza

| Nr | Moja | Poprawna | Status |
|----|------|----------|--------|
| 1  | A    | A        | ✅ |
| 2  | A    | B        | ❌ |
| 3  | C    | C        | ✅ |
| 4  | D    | D        | ✅ |
| 5  | C    | C        | ✅ |
| 6  | C    | C        | ✅ |
| 7  | B    | B        | ✅ |
| 8  | A    | A        | ✅ |
| 9  | A    | A        | ✅ |
| 10 | D    | D        | ✅ |
| 11 | A    | B        | ❌ |
| 12 | C    | B        | ❌ |
| 13 | C    | C        | ✅ |
| 14 | A    | A        | ✅ |
| 15 | D    | D        | ✅ |
| 16 | A    | D        | ❌ |
| 17 | A    | A        | ✅ |
| 18 | D    | D        | ✅ |
| 19 | C    | C        | ✅ |
| 20 | B    | B        | ✅ |

