# Szybka powtÃ³rka przed lekcjÄ…

Ultra-skondensowane przypomnienie najwaÅ¼niejszych rzeczy z kaÅ¼dej lekcji. UÅ¼yj przed kaÅ¼dÄ… lekcjÄ…, Å¼eby szybko odÅ›wieÅ¼yÄ‡ wiedzÄ™.

---

## ğŸ“Œ P6: Gradient Descent

**Co robi:** RÄ™czna implementacja regresji liniowej metodÄ… gradient descent

**Kluczowe elementy:**
- `bias = np.ones((m, 1))` â†’ dodanie wyrazu wolnego
- `gradient = (2/m) * X.T.dot(X.dot(weights) - Y)` â†’ obliczenie gradientu
- `weights = weights - eta * gradient` â†’ aktualizacja wag
- PÄ™tla 3000 iteracji â†’ uczenie modelu

**RÃ³wnanie:** `Y = w0 * 1 + w1 * X`

---

## ğŸ“Œ P7: Regresja liniowa scikit-learn

**Co robi:** Regresja liniowa na syntetycznych danych

**Kluczowe elementy:**
- `make_regression()` â†’ generowanie danych testowych
- `LinearRegression().fit()` â†’ trenowanie modelu
- `coef_`, `intercept_` â†’ parametry modelu
- `score()` â†’ RÂ² (wspÃ³Å‚czynnik determinacji)

**RÃ³wnanie:** `y = intercept_ + coef_[0] * x`

---

## ğŸ“Œ P8: Train/Test Split

**Co robi:** PodziaÅ‚ danych i ocena modelu na osobnych zbiorach

**Kluczowe elementy:**
- `train_test_split(test_size=0.25)` â†’ podziaÅ‚ 75/25
- `score(X_train)` vs `score(X_test)` â†’ wykrycie overfittingu
- Analiza bÅ‚Ä™dÃ³w: `error = y_test - y_pred`
- Histogram bÅ‚Ä™dÃ³w â†’ rozkÅ‚ad powinien byÄ‡ normalny wokÃ³Å‚ zera

**ZÅ‚ota zasada:** Model trenuje na train, ocenia na test!

---

## ğŸ“Œ P9: Rzeczywiste dane + EDA

**Co robi:** PeÅ‚ny pipeline: eksploracja â†’ feature engineering â†’ modelowanie

**Kluczowe elementy:**
- `read_csv()` â†’ wczytanie danych
- `drop_duplicates()` â†’ usuwanie duplikatÃ³w
- `get_dummies(drop_first=True)` â†’ one-hot encoding
- `corr()` â†’ macierz korelacji
- `mean_absolute_error()` â†’ metryka MAE

**Pipeline:**
1. EDA (`info()`, `describe()`, `value_counts()`)
2. Czyszczenie (duplikaty, braki)
3. Feature engineering (one-hot encoding)
4. Analiza korelacji
5. Train/test split
6. Trenowanie i ocena

---

## ğŸ“Œ P10: OLS statsmodels + selekcja zmiennych

**Co robi:** Model OLS z analizÄ… statystycznÄ… i backward elimination

**Kluczowe elementy:**
- `pd.get_dummies().values.astype(float)` â†’ przygotowanie danych
- `sm.add_constant()` â†’ dodanie intercept
- `sm.OLS().fit()` â†’ model OLS
- `ols.summary()` â†’ statystyki (p-value, RÂ²)
- Backward elimination â†’ usuwanie nieistotnych zmiennych (p â‰¥ 0.05)

**Interpretacja p-value:**
- **p < 0.05** â†’ istotna statystycznie âœ…
- **p â‰¥ 0.05** â†’ nieistotna (usuÅ„) âŒ

**Proces selekcji:**
1. PeÅ‚ny model â†’ sprawdÅº p-value
2. UsuÅ„ zmiennÄ… z najwyÅ¼szym p â‰¥ 0.05
3. PowtÃ³rz dla nowego modelu

---

## ğŸ”„ PowtarzajÄ…ce siÄ™ koncepty (wszystkie lekcje)

### Importy (standardowe)
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
```

### Konfiguracja
```python
np.random.seed(42)
sns.set(font_scale=1.3)
```

### One-Hot Encoding
```python
df_dummies = pd.get_dummies(df, drop_first=True)
```

### Train/Test Split
```python
X_train, X_test, y_train, y_test = train_test_split(
    data, target, test_size=0.2, random_state=42
)
```

### Model regresji
```python
regressor = LinearRegression()
regressor.fit(X_train, y_train)
score_train = regressor.score(X_train, y_train)
score_test = regressor.score(X_test, y_test)
```

---

## âš¡ Szybkie przypomnienie przed lekcjÄ…

**Przed P6:** PamiÄ™taj o reshape danych i dodaniu biasu

**Przed P7:** `make_regression()` do generowania danych, `score()` zwraca RÂ²

**Przed P8:** Zawsze dziel dane na train/test przed trenowaniem!

**Przed P9:** EDA â†’ czyszczenie â†’ encoding â†’ modelowanie

**Przed P10:** `drop_first=True` w get_dummies, `.astype(float)` przed statsmodels, p-value < 0.05 = istotna

---

## ğŸ¯ NajwaÅ¼niejsze zasady

1. **random_state=42** â†’ powtarzalnoÅ›Ä‡
2. **train/test split** â†’ zawsze przed trenowaniem
3. **drop_first=True** â†’ unika kolinearnoÅ›ci
4. **EDA przed modelowaniem** â†’ zrozum dane
5. **score(train) vs score(test)** â†’ wykrycie overfittingu
6. **p-value < 0.05** â†’ zmienna istotna

---

> **UÅ¼ycie:** Przeczytaj sekcjÄ™ dla danej lekcji przed zajÄ™ciami. PeÅ‚ne wyjaÅ›nienia w summary_p*.md, szczegÃ³Å‚y techniczne w cheat_sheet.md.
