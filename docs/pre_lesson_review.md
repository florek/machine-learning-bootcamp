# Szybka powtÃ³rka przed lekcjÄ…

Ultra-skondensowane przypomnienie najwaÅ¼niejszych rzeczy z kaÅ¼dej lekcji. UÅ¼yj przed kaÅ¼dÄ… lekcjÄ…, Å¼eby szybko odÅ›wieÅ¼yÄ‡ wiedzÄ™.

---

## ğŸ“Œ P6: Gradient Descent

**Co robi:** RÄ™czna implementacja regresji liniowej metodÄ… gradient descent

**Kluczowe elementy:**
- `bias = np.ones((m, 1))` â†’ dodanie wyrazu wolnego
- `gradient = (2/m) * X.T.dot(X.dot(weights) - Y)` â†’ obliczenie gradientu
- `weights = weights - eta * gradient` â†’ aktualizacja wag
- PÄ™tla 3000 iteracji â†’ uczenie modelu

**RÃ³wnanie:** `Y = w0 * 1 + w1 * X`

---

## ğŸ“Œ P7: Regresja liniowa scikit-learn

**Co robi:** Regresja liniowa na syntetycznych danych

**Kluczowe elementy:**
- `make_regression()` â†’ generowanie danych testowych
- `LinearRegression().fit()` â†’ trenowanie modelu
- `coef_`, `intercept_` â†’ parametry modelu
- `score()` â†’ RÂ² (wspÃ³Å‚czynnik determinacji)

**RÃ³wnanie:** `y = intercept_ + coef_[0] * x`

---

## ğŸ“Œ P8: Train/Test Split

**Co robi:** PodziaÅ‚ danych i ocena modelu na osobnych zbiorach

**Kluczowe elementy:**
- `train_test_split(test_size=0.25)` â†’ podziaÅ‚ 75/25
- `score(X_train)` vs `score(X_test)` â†’ wykrycie overfittingu
- Analiza bÅ‚Ä™dÃ³w: `error = y_test - y_pred`
- Histogram bÅ‚Ä™dÃ³w â†’ rozkÅ‚ad powinien byÄ‡ normalny wokÃ³Å‚ zera

**ZÅ‚ota zasada:** Model trenuje na train, ocenia na test!

---

## ğŸ“Œ P9: Rzeczywiste dane + EDA

**Co robi:** PeÅ‚ny pipeline: eksploracja â†’ feature engineering â†’ modelowanie

**Kluczowe elementy:**
- `read_csv()` â†’ wczytanie danych
- `drop_duplicates()` â†’ usuwanie duplikatÃ³w
- `get_dummies(drop_first=True)` â†’ one-hot encoding
- `corr()` â†’ macierz korelacji
- `mean_absolute_error()` â†’ metryka MAE

**Pipeline:**
1. EDA (`info()`, `describe()`, `value_counts()`)
2. Czyszczenie (duplikaty, braki)
3. Feature engineering (one-hot encoding)
4. Analiza korelacji
5. Train/test split
6. Trenowanie i ocena

---

## ğŸ“Œ P10: OLS statsmodels + selekcja zmiennych

**Co robi:** Model OLS z analizÄ… statystycznÄ… i rÄ™cznÄ… backward elimination

**Kluczowe elementy:**
- `pd.get_dummies().values.astype(float)` â†’ przygotowanie danych
- `sm.add_constant()` â†’ dodanie intercept
- `sm.OLS().fit()` â†’ model OLS
- `ols.summary()` â†’ statystyki (p-value, RÂ²)
- RÄ™czna backward elimination â†’ krok po kroku usuwanie nieistotnych zmiennych

**Interpretacja p-value:**
- **p < 0.05** â†’ istotna statystycznie âœ…
- **p â‰¥ 0.05** â†’ nieistotna (usuÅ„) âŒ

**Proces selekcji:**
1. PeÅ‚ny model â†’ sprawdÅº p-value
2. UsuÅ„ zmiennÄ… z najwyÅ¼szym p â‰¥ 0.05 (rÄ™cznie)
3. PowtÃ³rz dla nowego modelu

---

## ğŸ“Œ P11: Automatyczna backward elimination

**Co robi:** Automatyczna selekcja zmiennych w pÄ™tli while

**Kluczowe elementy:**
- `while True:` â†’ automatyczna pÄ™tla eliminacji
- `max(ols.pvalues)` â†’ najwyÅ¼sze p-value
- `np.argmax()` â†’ indeks zmiennej z najwyÅ¼szym p-value
- `np.delete(array, idx, axis=1)` â†’ usuniÄ™cie kolumny
- `ols.save('model.pickle')` â†’ zapis modelu do pliku

**Proces automatyczny:**
1. Dopasuj model â†’ znajdÅº max p-value
2. JeÅ›li max p-value > 0.05 â†’ usuÅ„ zmiennÄ…
3. PowtÃ³rz, dopÃ³ki wszystkie zmienne sÄ… istotne (p â‰¤ 0.05)

**RÃ³Å¼nica od P10:**
- P10: rÄ™czne usuwanie (3 kroki)
- P11: automatyczna pÄ™tla (dziaÅ‚a dla dowolnej liczby zmiennych)

---

## ğŸ“Œ P12: Regresja wielomianowa

**Co robi:** Uchwycenie nieliniowej zaleÅ¼noÅ›ci (wielomianowej) przez rozszerzenie cech (X, XÂ², XÂ³) i zwykÅ‚Ä… regresjÄ™ liniowÄ…

**Kluczowe elementy:**
- `np.random.seed(42)` â†’ powtarzalnoÅ›Ä‡ danych i szumu
- `X.reshape(n, 1)` â†’ ksztaÅ‚t 2D (prÃ³bki Ã— cechy) dla scikit-learn
- Regresja liniowa na jednej cesze â†’ sÅ‚abe RÂ² przy zaleÅ¼noÅ›ci wielomianowej
- `PolynomialFeatures(degree=k)` â†’ tworzy cechy 1, X, XÂ², â€¦; potem `LinearRegression`
- `r2_score(y, y_pred)` / `score()` â†’ ocena dopasowania

**Zasada:** Model pozostaje liniowy wzglÄ™dem parametrÃ³w; nieliniowoÅ›Ä‡ wynika z transformacji cech. Przy wysokim stopniu i maÅ‚ej liczbie danych â€“ ryzyko przeuczenia (regularyzacja lub niÅ¼szy stopieÅ„).

---

## ğŸ”„ PowtarzajÄ…ce siÄ™ koncepty (wszystkie lekcje)

### Importy (standardowe)
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
```

### Konfiguracja
```python
np.random.seed(42)
sns.set(font_scale=1.3)
```

### One-Hot Encoding
```python
df_dummies = pd.get_dummies(df, drop_first=True)
```

### Train/Test Split
```python
X_train, X_test, y_train, y_test = train_test_split(
    data, target, test_size=0.2, random_state=42
)
```

### Model regresji
```python
regressor = LinearRegression()
regressor.fit(X_train, y_train)
score_train = regressor.score(X_train, y_train)
score_test = regressor.score(X_test, y_test)
```

---

## âš¡ Szybkie przypomnienie przed lekcjÄ…

**Przed P6:** PamiÄ™taj o reshape danych i dodaniu biasu

**Przed P7:** `make_regression()` do generowania danych, `score()` zwraca RÂ²

**Przed P8:** Zawsze dziel dane na train/test przed trenowaniem!

**Przed P9:** EDA â†’ czyszczenie â†’ encoding â†’ modelowanie

**Przed P10:** `drop_first=True` w get_dummies, `.astype(float)` przed statsmodels, p-value < 0.05 = istotna

**Przed P11:** `while True` z `break`, `np.argmax()` do znajdowania indeksu, `np.delete()` do usuwania kolumn

**Przed P12:** `reshape(n, 1)` dla jednej cechy, regresja wielomianowa = rozszerzenie cech + LinearRegression, RÂ² przy nieliniowej zaleÅ¼noÅ›ci

---

## ğŸ¯ NajwaÅ¼niejsze zasady

1. **random_state=42** â†’ powtarzalnoÅ›Ä‡
2. **train/test split** â†’ zawsze przed trenowaniem
3. **drop_first=True** â†’ unika kolinearnoÅ›ci
4. **EDA przed modelowaniem** â†’ zrozum dane
5. **score(train) vs score(test)** â†’ wykrycie overfittingu
6. **p-value < 0.05** â†’ zmienna istotna

---

> **UÅ¼ycie:** Przeczytaj sekcjÄ™ dla danej lekcji przed zajÄ™ciami. PeÅ‚ne wyjaÅ›nienia w summary_p6.md â€“ summary_p12.md, szczegÃ³Å‚y techniczne w cheat_sheet.md.
